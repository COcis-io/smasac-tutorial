{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demostrate to build a basic statistical features driven classifier on Crisis Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# coding: utf8\n",
    "\n",
    "# load required libraries\n",
    "import spacy\n",
    "import csv\n",
    "import sys\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy import sparse\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import string\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_recall_fscore_support as prfs_score\n",
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.cross_validation import cross_val_predict\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.externals import joblib\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "import codecs\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload Training and Test Data (if using a test file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_file = '../data/QFL.csv'\n",
    "test_file = '../data/CFL.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Upload a customised multi-lingual stopword dictionary\n",
    "\n",
    "data_json = json.load(codecs.open(\n",
    "    '../data/selected_lang_stopwords.json', 'r', 'utf-8'))\n",
    "\n",
    "s = set(data_json['en'])\n",
    "fs = frozenset(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a tokenizer and stemming\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def tokenize_and_stem(text):\n",
    "    \n",
    "    tokens = regexp_tokenize(text, pattern=r\"\\s|[\\.,:;'()?!]\", gaps=True)\n",
    "    # strip out punctuation and make lowercase\n",
    "    tokens = [token.lower().strip(string.punctuation)\n",
    "              for token in tokens if token.isalnum()]\n",
    "\n",
    "    # now stem the tokens\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload language model in spaCy- the natural language processing library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp_en = spacy.load('en')\n",
    "\n",
    "nlp = nlp_en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Statistical Feature extraction method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_statfeatures(tweet):\n",
    "    string = tweet\n",
    "    \n",
    "    dict_pos = dict()\n",
    "\n",
    "    doc = nlp(string.decode('utf-8'))\n",
    "\n",
    "    for token in doc:\n",
    "\n",
    "        if dict_pos.has_key(token.pos_):\n",
    "\n",
    "            dict_pos[token.pos_] = dict_pos[token.pos_] + 1\n",
    "\n",
    "        else:\n",
    "\n",
    "            dict_pos[token.pos_] = 1\n",
    "\n",
    "    pronouns = 0\n",
    "    nouns = 0\n",
    "    verbs = 0\n",
    "\n",
    "    if 'PRON' in dict_pos:\n",
    "        pronouns = dict_pos['PRON']\n",
    "\n",
    "    if 'NOUN' in dict_pos:\n",
    "        nouns = dict_pos['NOUN']\n",
    "\n",
    "    if 'PROPN' in dict_pos:\n",
    "        nouns = dict_pos['PROPN'] + nouns\n",
    "\n",
    "    if 'VERB' in dict_pos:\n",
    "        verbs = dict_pos['VERB']\n",
    "\n",
    "    tweet_length = len(string)\n",
    "    token_count = len(re.findall(r'\\w+', string))\n",
    "    numHashTag = len([i for i in string.split() if i.startswith(\"#\")])\n",
    " \n",
    "    return pronouns, nouns, verbs, tweet_length, token_count, numHashTag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = ['Document', 'NumberOfNouns', 'NumberOfVerbs', 'NumberOfPronouns',\n",
    "         'TweetLength', 'NumberOfWords', 'NumberOfHashTag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first pass loading the training data\n",
    "\n",
    "with open(train_file) as fline:\n",
    "    \n",
    "    data_csv = list(csv.reader(fline, delimiter=\"\\t\", quoting=csv.QUOTE_NONE))\n",
    "\n",
    "    data = np.array(data_csv[0:])\n",
    "    \n",
    "    data_train = np.empty(shape=(len(data_csv),7),dtype='|S300')\n",
    "    \n",
    "    # the labels/class for each document\n",
    "    #label_train = data[:, 4].astype(np.float32)\n",
    "    label_train = data[:, 2].astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating the statistical features of the training data\n",
    "for i in range(0,data.shape[0]):\n",
    "    \n",
    "    pronouns, nouns, verbs, tweet_length, token_count, numHashTag = extract_statfeatures(data[i][1])\n",
    "    \n",
    "    data_train[i][0] = data[i][1]\n",
    "    \n",
    "    data_train[i][1] = nouns\n",
    "    data_train[i][2] = verbs\n",
    "    data_train[i][3] = pronouns\n",
    "    data_train[i][4] = tweet_length\n",
    "    data_train[i][5] = token_count\n",
    "    data_train[i][6] = numHashTag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frame_train = pd.DataFrame(data_train, columns=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>NumberOfNouns</th>\n",
       "      <th>NumberOfVerbs</th>\n",
       "      <th>NumberOfPronouns</th>\n",
       "      <th>TweetLength</th>\n",
       "      <th>NumberOfWords</th>\n",
       "      <th>NumberOfHashTag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT if you are older than 9 #bigwet #getinvolved</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#qldfloods lmao. Whoever put those things were...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>122</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#QLD  Police: #bigwet Bruce H'Way near Mobil S...</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>107</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @ACPMH: Check out @beyondblue looking after...</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>140</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HANDY list of contacts and numbers. Have a loo...</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>125</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Document NumberOfNouns  \\\n",
       "0    RT if you are older than 9 #bigwet #getinvolved             4   \n",
       "1  #qldfloods lmao. Whoever put those things were...             5   \n",
       "2  #QLD  Police: #bigwet Bruce H'Way near Mobil S...            12   \n",
       "3  RT @ACPMH: Check out @beyondblue looking after...            10   \n",
       "4  HANDY list of contacts and numbers. Have a loo...             7   \n",
       "\n",
       "  NumberOfVerbs NumberOfPronouns TweetLength NumberOfWords NumberOfHashTag  \n",
       "0             2                1          47             9               2  \n",
       "1             3                1         122            22               1  \n",
       "2             1                0         107            18               2  \n",
       "3             2                1         140            21               2  \n",
       "4             6                1         125            23               2  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just check how the data frame looks like\n",
    "\n",
    "frame_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform the data to vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer='word',tokenizer=tokenize_and_stem,\n",
    "                                 stop_words=fs, lowercase=True, ngram_range=(1, 1), max_features=40000)\n",
    "\n",
    "doc_vectorize = vectorizer.fit_transform(frame_train.Document)\n",
    "tf_transform = TfidfTransformer()\n",
    "tf_vectorize = tf_transform.fit_transform(doc_vectorize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train_stack = sp.sparse.hstack((tf_vectorize, frame_train[['NumberOfNouns', 'NumberOfVerbs', 'NumberOfPronouns', 'TweetLength',\n",
    "                                                  'NumberOfWords', 'NumberOfHashTag']].values.astype(np.float32)), format='csr')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf-vectorizer shape:  (556, 1485)\n",
      "overall feature stack shape:  (556, 1491)\n"
     ]
    }
   ],
   "source": [
    "# confirm the stack shape\n",
    "print 'tf-vectorizer shape: ',tf_vectorize.shape\n",
    "print 'overall feature stack shape: ',data_train_stack.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.0: 278, 1.0: 278}\n"
     ]
    }
   ],
   "source": [
    "# check the data distribution class vice\n",
    "unique, counts = np.unique(label_train, return_counts=True)\n",
    "print dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declare the classifier and fit on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define SVC kernel and fit\n",
    "\n",
    "svc = SVC(kernel='linear', degree=3, gamma='auto', tol=0.001)\n",
    "\n",
    "svc.fit(data_train_stack, label_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declare calling methods for the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the methods to test on a text of test tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_test_tweet_features(test_tweet):\n",
    "    \n",
    "    pronouns, nouns, verbs, tweet_length, token_count, numHashTag = extract_statfeatures(test_tweet)\n",
    "    \n",
    "    data_test = np.array([[test_tweet, nouns, verbs, pronouns,\n",
    "                               tweet_length, token_count, numHashTag]])\n",
    "    return data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify_tweet(test_tweet):\n",
    "\n",
    "    data_test = get_test_tweet_features(test_tweet)\n",
    "\n",
    "    frame_test = pd.DataFrame(data_test, columns=t)\n",
    "    \n",
    "    test_vectorize = vectorizer.transform(frame_test.Document)\n",
    "    test_tf_vectorize = tf_transform.transform(test_vectorize)\n",
    "\n",
    "    data_test_stack = sp.sparse.hstack((test_tf_vectorize, frame_test[['NumberOfNouns', 'NumberOfVerbs', 'NumberOfPronouns',\n",
    "                                                                 'TweetLength', 'NumberOfWords', 'NumberOfHashTag']].values.astype(np.float32)), format='csr')\n",
    "    \n",
    "    label_test_predict = svc.predict(data_test_stack)\n",
    "    \n",
    "    return json.dumps({'class': str(label_test_predict[0])})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End API call "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results a json response of 'class' indicating 1.0 (crisis related) or 0.0 (not related)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"class\": \"1.0\"}\n",
      "{\"class\": \"0.0\"}\n"
     ]
    }
   ],
   "source": [
    "print classify_tweet('The car got drowned in the heavy flood.')\n",
    "print classify_tweet('The building collapsed in the heavy rainfall.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#regexp_tokenize('Pygmy whales washed up in huge swells http://t.co/gt1wgSn9 @abcnews #bigwet', pattern=r\"\\s|[\\.,:;'()?!]\", gaps=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Transformation of the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a BabelNet Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "import urllib\n",
    "import json\n",
    "import gzip\n",
    "\n",
    "from StringIO import StringIO\n",
    "\n",
    "# BabelNet Key : http://babelnet.org/ or Babelfy Key : http://babelfy.org/\n",
    "key  = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define methods for calling annotation/disambiguation API, sense gathering, and semantic neighbours (hypernyms) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# text annotation, returns a list of annotated SynsetIDs\n",
    "def get_synsetID_text(text):\n",
    "    \n",
    "    service_url = 'https://babelfy.io/v1/disambiguate'\n",
    "\n",
    "    #text = 'BabelNet is both a multilingual encyclopedic dictionary and a semantic network'\n",
    "    lang = 'EN'\n",
    "    \n",
    "    synset_list = []\n",
    "    \n",
    "    params = {\n",
    "        'text' : text,\n",
    "        'lang' : lang,\n",
    "        'key'  : key\n",
    "    }\n",
    "\n",
    "    url = service_url + '?' + urllib.urlencode(params)\n",
    "    request = urllib2.Request(url)\n",
    "    request.add_header('Accept-encoding', 'gzip')\n",
    "    response = urllib2.urlopen(request)\n",
    "\n",
    "    if response.info().get('Content-Encoding') == 'gzip':\n",
    "        buf = StringIO( response.read())\n",
    "        f = gzip.GzipFile(fileobj=buf)\n",
    "        data = json.loads(f.read())\n",
    "\n",
    "        # retrieving data\n",
    "        for result in data:\n",
    "            # retrieving token fragment\n",
    "            '''tokenFragment = result.get('tokenFragment')\n",
    "            tfStart = tokenFragment.get('start')\n",
    "            tfEnd = tokenFragment.get('end')\n",
    "            print str(tfStart) + \"\\t\" + str(tfEnd)'''\n",
    "\n",
    "\n",
    "            # retrieving char fragment\n",
    "            charFragment = result.get('charFragment')\n",
    "            cfStart = charFragment.get('start')\n",
    "            cfEnd = charFragment.get('end')\n",
    "            #print str(cfStart) + \"\\t\" + str(cfEnd)\n",
    "\n",
    "\n",
    "            # retrieving BabelSynset ID\n",
    "            synsetId = result.get('babelSynsetID')\n",
    "            synset_list.append([text[cfStart:cfEnd+1],synsetId])\n",
    "            #print text[cfStart:cfEnd+1],' ',synsetId\n",
    "            \n",
    "    return synset_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# retrieve the sense for each SynsetID, returns a dictionary for associated senses in English for each SynsetID\n",
    "def get_babel_sense(id):\n",
    "    \n",
    "    service_url = 'https://babelnet.io/v5/getSynset'\n",
    "\n",
    "    lang ='EN'\n",
    "    sense_dict = dict()\n",
    "    params = {\n",
    "        'id' : id,\n",
    "        'key'  : key,\n",
    "        'targetLang' : lang\n",
    "    }\n",
    "\n",
    "    url = service_url + '?' + urllib.urlencode(params)\n",
    "    request = urllib2.Request(url)\n",
    "    request.add_header('Accept-encoding', 'gzip')\n",
    "    response = urllib2.urlopen(request)\n",
    "\n",
    "    if response.info().get('Content-Encoding') == 'gzip':\n",
    "        buf = StringIO( response.read())\n",
    "        f = gzip.GzipFile(fileobj=buf)\n",
    "        data = json.loads(f.read())\n",
    "        \n",
    "        # retrieving BabelSense data\n",
    "        senses = data['senses']\n",
    "        for result in senses:\n",
    "            \n",
    "            #language = result.get('language')\n",
    "            #print language.encode('utf-8') + \"\\t\" + str(lemma.encode('utf-8'))\n",
    "            if result['properties']['fullLemma'].lower() not in sense_dict:\n",
    "                sense_dict[result['properties']['fullLemma'].lower()] = 1\n",
    "            #print result['properties']['fullLemma']\n",
    "            \n",
    "    return sense_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# retrieve the neighbours of each SynsetID, in this case we extract Hypernyms to broaden the context\n",
    "def get_babel_neighbours(id):\n",
    "    \n",
    "    service_url = 'https://babelnet.io/v5/getOutgoingEdges'\n",
    "\n",
    "    #id = 'bn:00007287n'\n",
    "    hyper_list = []\n",
    "    lang = 'EN'\n",
    "    params = {\n",
    "        'id' : id,\n",
    "        'key'  : key\n",
    "    }\n",
    "\n",
    "    url = service_url + '?' + urllib.urlencode(params)\n",
    "    request = urllib2.Request(url)\n",
    "    request.add_header('Accept-encoding', 'gzip')\n",
    "    response = urllib2.urlopen(request)\n",
    "\n",
    "    if response.info().get('Content-Encoding') == 'gzip':\n",
    "        buf = StringIO( response.read())\n",
    "        f = gzip.GzipFile(fileobj=buf)\n",
    "        data = json.loads(f.read())\n",
    "        \n",
    "        # retrieving Edges data\n",
    "        for result in data:\n",
    "            \n",
    "            target = result['target']\n",
    "            \n",
    "            language = result['language']\n",
    "\n",
    "            # retrieving BabelPointer data\n",
    "            pointer = result['pointer']\n",
    "            relation = pointer.get('name')\n",
    "            group = pointer.get('relationGroup')\n",
    "\n",
    "            # Types of relationGroup: HYPERNYM,  HYPONYM, MERONYM, HOLONYM, OTHER\n",
    "            #if ('hypernym' in group.lower() or 'hyponym' in group.lower()):\n",
    "\n",
    "            if ('hypernym' in group.lower()) and str(language)=='EN':\n",
    "                    #print (str(language) + \"\\t\" + str(target) + \"\\t\" + str(relation) + \"\\t\" + str(group))\n",
    "                    hyper_list.append([id, str(target), str(relation)])\n",
    "                    \n",
    "                    \n",
    "            #elif ('antonym' in relation.lower()):\n",
    "             #       print (str(language) + \"\\t\" + str(target) + \"\\t\" + str(relation) + \"\\t\" + str(group))\n",
    "\n",
    "    return hyper_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantify a sample tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test the use case\n",
    "\n",
    "tweet = 'The building collapsed in the heavy rainfall.'\n",
    "overall_concept = tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['building', u'bn:00084198v'],\n",
       " ['collapsed', u'bn:00085281v'],\n",
       " ['heavy', u'bn:00104050a'],\n",
       " ['rainfall', u'bn:00066032n']]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get SynsetIDs in the text\n",
    "synset_list_return = get_synsetID_text(tweet)\n",
    "\n",
    "synset_list_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get Sense in English for each Syset ID\n",
    "for i in range(0, len(synset_list_return)):\n",
    "    \n",
    "    sense_dictonary = get_babel_sense(synset_list_return[i][1])\n",
    "    \n",
    "    for j in sense_dictonary.keys():\n",
    "        \n",
    "        overall_concept = overall_concept.strip() + ' ' + j.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'The building collapsed in the heavy rainfall. make produce build construct collapse heavy raining intensity_frequency_and_duration wettest_spot_on_earth \\U0001f326 \\U0001f327 rained rainstorm pissing_it_down rainwater pluviophile heavy_rain_(meteorology) rainy rains hyetal rain_water rainfall \\u26c6 wettest_places_on_earth rain rain_storm rain_measurement rainiest torrential_rain raindrops rainfall_intensity'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get neighbour SynsetIDs (hypernyms) for each SynsetID\n",
    "\n",
    "for i in range(0, len(synset_list_return)):\n",
    "    \n",
    "    neighbour_list = get_babel_neighbours(synset_list_return[i][1])\n",
    "    \n",
    "    for j in range(0, len(neighbour_list)):\n",
    "        \n",
    "        # Get sense of each neighbouring Synset ID\n",
    "        \n",
    "        neighbour_sense_dictonary = get_babel_sense(neighbour_list[j][1])\n",
    "        \n",
    "        for k in neighbour_sense_dictonary.keys():\n",
    "        \n",
    "            overall_concept = overall_concept.strip() + ' ' + k.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'The building collapsed in the heavy rainfall. make produce build construct collapse heavy raining intensity_frequency_and_duration wettest_spot_on_earth \\U0001f326 \\U0001f327 rained rainstorm pissing_it_down rainwater pluviophile heavy_rain_(meteorology) rainy rains hyetal rain_water rainfall \\u26c6 wettest_places_on_earth rain rain_storm rain_measurement rainiest torrential_rain raindrops rainfall_intensity make create fold_up fold turn_up precipitation_measurement cloud_condensation praecipitation atmospheric_precipitation downfall hydrometeor cumulonimbus_praecipitatio precipitaion precipitation_(meteorology) classification_of_clouds atmospheric_hydrometeor hydrometeors precipitaiton precipitation annual_precipitation convectional_precipitation'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_concept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Semantic Enrichment Based Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading already created model\n",
    "\n",
    "filename = \"../data/SVC_classifier_model.pkl\"\n",
    "loaded_model = pickle.load(open(filename, \"rb\"))\n",
    "\n",
    "filename_vectorize = \"../data/vectorizer.pkl\"\n",
    "filename_tf_transform = \"../data/tf_transform.pkl\"\n",
    "\n",
    "loaded_vectorizer = pickle.load(open(filename_vectorize, \"rb\"))\n",
    "loaded_tf_transform = pickle.load(open(filename_tf_transform, \"rb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def semantic_enrichment(tweet):\n",
    "    \n",
    "    enriched_tweet = tweet\n",
    "    \n",
    "    synset_list_return = get_synsetID_text(tweet)\n",
    "    \n",
    "    # Get Sense in English for each Syset ID\n",
    "    for i in range(0, len(synset_list_return)):\n",
    "\n",
    "        sense_dictonary = get_babel_sense(synset_list_return[i][1])\n",
    "\n",
    "        for j in sense_dictonary.keys():\n",
    "\n",
    "            enriched_tweet = enriched_tweet.strip() + ' ' + j.strip()\n",
    "            \n",
    "    # Get neighbour SynsetIDs (hypernyms) for each SynsetID\n",
    "\n",
    "    for i in range(0, len(synset_list_return)):\n",
    "\n",
    "        neighbour_list = get_babel_neighbours(synset_list_return[i][1])\n",
    "\n",
    "        for j in range(0, len(neighbour_list)):\n",
    "\n",
    "            # Get sense of each neighbouring Synset ID\n",
    "\n",
    "            neighbour_sense_dictonary = get_babel_sense(neighbour_list[j][1])\n",
    "\n",
    "            for k in neighbour_sense_dictonary.keys():\n",
    "\n",
    "                enriched_tweet = enriched_tweet.strip() + ' ' + k.strip()\n",
    "                \n",
    "    return enriched_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_sem_tweet = 'The building collapsed in the heavy rainfall.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "enriched_tweet_response = semantic_enrichment(test_sem_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pronouns, nouns, verbs, tweet_length, token_count, numHashTag = extract_statfeatures(test_sem_tweet)\n",
    "    \n",
    "data_sem_test = np.array([[enriched_tweet_response, nouns, verbs, pronouns, tweet_length, token_count, numHashTag]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[u'The building collapsed in the heavy rainfall. make produce build construct collapse heavy raining intensity_frequency_and_duration wettest_spot_on_earth \\U0001f326 \\U0001f327 rained rainstorm pissing_it_down rainwater pluviophile heavy_rain_(meteorology) rainy rains hyetal rain_water rainfall \\u26c6 wettest_places_on_earth rain rain_storm rain_measurement rainiest torrential_rain raindrops rainfall_intensity make create fold_up fold turn_up precipitation_measurement cloud_condensation praecipitation atmospheric_precipitation downfall hydrometeor cumulonimbus_praecipitatio precipitaion precipitation_(meteorology) classification_of_clouds atmospheric_hydrometeor hydrometeors precipitaiton precipitation annual_precipitation convectional_precipitation',\n",
       "        u'2', u'1', u'0', u'45', u'7', u'0']], dtype='<U736')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sem_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"class\": \"1.0\"}'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame_sem_test = pd.DataFrame(data_sem_test, columns=t)\n",
    "\n",
    "test_sem_vectorize = loaded_vectorizer.transform(frame_sem_test.Document)\n",
    "test_sem_tf_vectorize = loaded_tf_transform.transform(test_sem_vectorize)\n",
    "\n",
    "data_sem_test_stack = sp.sparse.hstack((test_sem_tf_vectorize, frame_sem_test[['NumberOfNouns', 'NumberOfVerbs', 'NumberOfPronouns',\n",
    "                                                             'TweetLength', 'NumberOfWords', 'NumberOfHashTag']].values.astype(np.float32)), format='csr')\n",
    "\n",
    "label_sem_test_predict = loaded_model.predict(data_sem_test_stack)\n",
    "\n",
    "json.dumps({'class': str(label_sem_test_predict[0])})"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
